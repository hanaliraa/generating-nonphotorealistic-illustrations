
@article{lu_non-photorealistic_2013,
	series = {6th {International} {Congress} on {Image} and {Signal} {Processing} ({CISP} 2013)},
	title = {A {Non}-photorealistic {Rendering} {Algorithm} {For} {Cartoons}},
	abstract = {In this paper, a non-photorealistic rendering technology of cartoons is studied. A new algorithm which can automatically generate cartoon paintings is proposed. This method needs no user’s participation and no drawing skills. Firstly, by using the bilateral filtering, the abstract visually appealing and restrain minor features can be highlighted. Second, the abstract image is quantified to imitate the coloring style of the cartoon. Third, FDOG filter is used to draw contour lines of the abstract image to further improve visualization. Last, the quantized image and the contour image are fused to produce an image with the cartoons. The result shows that cartoons generated by this algorithm have characters of art aesthetic and a better visualization.},
	language = {en},
	author = {Lu, Li-wen and Pu, Yuan-yuan and Zhang, Heng and Xu, Dan},
	year = {2013},
	pages = {6},
	file = {A Non-photorealistic Rendering Algorithm For cartoons.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\A Non-photorealistic Rendering Algorithm For cartoons.pdf:application/pdf},
}

@article{nur_cartoon_2013,
	title = {Cartoon {Effect} and {Ambient} {Illumination} {Based} {Depth} {Perception} {Assessment} of {3D} {Video}},
	volume = {7},
	abstract = {Monitored 3-Dimensional (3D) video experience can be utilized as “feedback information” to fine tune the service parameters for providing a better service to the demanding 3D service customers. The 3D video experience which includes both video quality and depth perception is influenced by several contextual and content related factors (e.g., ambient illumination condition, content characteristics, etc) due to the complex nature of the 3D video. Therefore, effective factors on this experience should be utilized while assessing it. In this paper, structural information of the depth map sequences of the 3D video is considered as content related factor effective on the depth perception assessment. Cartoon-like filter is utilized to abstract the significant depth levels in the depth map sequences to determine the structural information. Moreover, subjective experiments are conducted using 3D videos associated with cartoon-like depth map sequences to investigate the effectiveness of ambient illumination condition, which is a contextual factor, on depth perception. Using the knowledge gained through this study, 3D video experience metrics can be developed to deliver better service to the 3D video service users.},
	language = {en},
	number = {7},
	author = {Nur, G},
	year = {2013},
	pages = {4},
	file = {cartoon-effect-and-ambient-illumination-based-depth-perception-assessment-of-3d-video.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\cartoon-effect-and-ambient-illumination-based-depth-perception-assessment-of-3d-video.pdf:application/pdf},
}

@article{hornung_character_2007,
	title = {Character animation from {2D} pictures and {3D} motion data},
	volume = {26},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/1189762.1189763},
	doi = {10.1145/1189762.1189763},
	abstract = {This article presents a new method to animate photos of 2D characters using 3D motion capture data. Given a single image of a person or essentially human-like subject, our method transfers the motion of a 3D skeleton onto the subject's 2D shape in image space, generating the impression of a realistic movement. We present robust solutions to reconstruct a projective camera model and a 3D model pose which matches best to the given 2D image. Depending on the reconstructed view, a 2D shape template is selected which enables the proper handling of occlusions. After fitting the template to the character in the input image, it is deformed as-rigid-as-possible by taking the projected 3D motion data into account. Unlike previous work, our method thereby correctly handles projective shape distortion. It works for images from arbitrary views and requires only a small amount of user interaction. We present animations of a diverse set of human (and nonhuman) characters with different types of motions, such as walking, jumping, or dancing.},
	language = {en},
	number = {1},
	urldate = {2021-11-01},
	journal = {ACM Transactions on Graphics},
	author = {Hornung, Alexander and Dekkers, Ellen and Kobbelt, Leif},
	month = jan,
	year = {2007},
	pages = {1},
	file = {Character Animation from 2D Pictures and 3D Motion Data.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Character Animation from 2D Pictures and 3D Motion Data.pdf:application/pdf},
}

@article{lee_context-aware_nodate,
	title = {Context-aware {Synthesis} and {Placement} of {Object} {Instances}},
	abstract = {Learning to insert an object instance into an image in a semantically coherent manner is a challenging and interesting problem. Solving it requires (a) determining a location to place an object in the scene and (b) determining its appearance at the location. Such an object insertion model can potentially facilitate numerous image editing and scene parsing applications. In this paper, we propose an end-to-end trainable neural network for the task of inserting an object instance mask of a speciﬁed class into the semantic label map of an image. Our network consists of two generative modules where one determines where the inserted object mask should be (i.e., location and scale) and the other determines what the object mask shape (and pose) should look like. The two modules are connected together via a spatial transformation network and jointly trained. We devise a learning procedure that leverage both supervised and unsupervised data and show our model can insert an object at diverse locations with various appearances. We conduct extensive experimental validations with comparisons to strong baselines to verify the effectiveness of the proposed network. Code is available at https: //github.com/NVlabs/Instance\_Insertion.},
	language = {en},
	author = {Lee, Donghoon and Liu, Sifei and Gu, Jinwei and Liu, Ming-Yu and Yang, Ming-Hsuan and Kautz, Jan},
	pages = {11},
	file = {Context-Aware Synthesis and Placement of Object Instances.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Context-Aware Synthesis and Placement of Object Instances.pdf:application/pdf},
}

@article{lee_generation_2019,
	title = {Generation of cartoon-style bas-reliefs from photographs},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-017-5343-0},
	doi = {10.1007/s11042-017-5343-0},
	abstract = {This paper describes a new algorithm that generates a cartoon-style bas-relief surface from photographs of general scenes. Most previous methods for bas-relief generation have focused on accurate restoration of input 3D models on a background plane. The generation of bas-reliefs with artistic effects has rarely been studied. Considering that nonphotorealistic rendering (NPR) techniques are currently very popular and 3D printing technology is developing rapidly, extending NPR techniques to the generation of a bas-relief surface with artistic effects is natural and valuable. Furthermore, cartoon is a basic non-realistic and artistic style familiar to general users. From this motivation, our method focuses on generating a cartoon-style bas-relief surface. We use the lens blur function of Google Camera, which is a smartphone application, to obtain a photograph and its depth map as inputs. Using coherent line drawing and histogram-based quantization methods, we construct a depth map that contains the salient features of given input scenes in abstract form. Displacement mapping from the depth map onto a thin plane generates a cartoon-style bas-relief. Experimental results show that our method generates bas-relief surfaces that contain the characteristics of cartoons, such as coherent border lines and quantized layers.},
	language = {en},
	number = {20},
	urldate = {2021-11-01},
	journal = {Multimedia Tools and Applications},
	author = {Lee, Seungchan and Sohn, Bong-Soo},
	month = oct,
	year = {2019},
	pages = {28391--28407},
	file = {Generation of cartoon-style bas-reliefs from photographs.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Generation of cartoon-style bas-reliefs from photographs.pdf:application/pdf},
}

@article{ellis_learning_2018,
	title = {Learning to {Infer} {Graphics} {Programs} from {Hand}-{Drawn} {Images}},
	url = {http://arxiv.org/abs/1707.09627},
	abstract = {We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of LATEX. The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are a speciﬁcation (spec) of what the graphics program needs to draw. We learn a model that uses program synthesis techniques to recover a graphics program from that spec. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network, measure similarity between drawings by use of similar high-level geometric structures, and extrapolate drawings.},
	language = {en},
	urldate = {2021-11-01},
	journal = {arXiv:1707.09627 [cs]},
	author = {Ellis, Kevin and Ritchie, Daniel and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
	month = oct,
	year = {2018},
	note = {arXiv: 1707.09627},
	keywords = {68T05, Computer Science - Artificial Intelligence},
	file = {Learning to Infer Graphics Programs from handrawn images.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Learning to Infer Graphics Programs from handrawn images.pdf:application/pdf},
}

@article{tucker_rebar_2017,
	title = {{REBAR}: {Low}-variance, unbiased gradient estimates for discrete latent variable models},
	shorttitle = {{REBAR}},
	url = {http://arxiv.org/abs/1703.07370},
	abstract = {Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al., 2016; Maddison et al., 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, unbiased gradient estimates. Then, we introduce a modiﬁcation to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better ﬁnal log-likelihood.},
	language = {en},
	urldate = {2021-11-01},
	journal = {arXiv:1703.07370 [cs, stat]},
	author = {Tucker, George and Mnih, Andriy and Maddison, Chris J. and Lawson, Dieterich and Sohl-Dickstein, Jascha},
	month = nov,
	year = {2017},
	note = {arXiv: 1703.07370},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {REBAR_ Low-variance, unbiased gradient estimates for discrete latent variable models.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\REBAR_ Low-variance, unbiased gradient estimates for discrete latent variable models.pdf:application/pdf},
}

@article{dade_toonify_nodate,
	title = {Toonify: {Cartoon} {Photo} {Effect} {Application}},
	abstract = {Toonify seeks to emulate the types of cel-shading effects offered by graphics engines in a lighthearted and userfriendly way.},
	language = {en},
	author = {Dade, Kevin},
	pages = {3},
	file = {Toonify-Cartoon Photo Effect Application.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Toonify-Cartoon Photo Effect Application.pdf:application/pdf},
}

@article{sbai_unsupervised_2019,
	title = {Unsupervised {Image} {Decomposition} in {Vector} {Layers}},
	url = {http://arxiv.org/abs/1812.05484},
	abstract = {Deep image generation is becoming a tool to enhance artists and designers creativity potential. In this paper, we aim at making the generation process more structured and easier to interact with. Inspired by vector graphics systems, we propose a new deep image reconstruction paradigm where the outputs are composed from simple layers, deﬁned by their color and a vector transparency mask. This presents a number of advantages compared to the commonly used convolutional network architectures. In particular, our layered decomposition allows simple user interaction, for example to update a given mask, or change the color of a selected layer. From a compact code, our architecture also generates vector images with a virtually inﬁnite resolution, the color at each point in an image being a parametric function of its coordinates. We validate the efﬁciency of our approach by comparing reconstructions with state-of-the-art baselines given similar memory resources on CelebA and ImageNet datasets. Most importantly, we demonstrate several applications of our new image representation obtained in an unsupervised manner, including editing, vectorization and image search.},
	language = {en},
	urldate = {2021-11-01},
	journal = {arXiv:1812.05484 [cs]},
	author = {Sbai, Othman and Couprie, Camille and Aubry, Mathieu},
	month = jul,
	year = {2019},
	note = {arXiv: 1812.05484},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Unsupervised Image Decomposition in Vector Layers.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Unsupervised Image Decomposition in Vector Layers.pdf:application/pdf},
}


@inproceedings{kang_coherent_2007,
	address = {San Diego, California},
	title = {Coherent line drawing},
	isbn = {978-1-59593-624-0},
	url = {http://dl.acm.org/citation.cfm?doid=1274871.1274878},
	doi = {10.1145/1274871.1274878},
	abstract = {This paper presents a non-photorealistic rendering technique that automatically generates a line drawing from a photograph. We aim at extracting a set of coherent, smooth, and stylistic lines that effectively capture and convey important shapes in the image. We ﬁrst develop a novel method for constructing a smooth direction ﬁeld that preserves the ﬂow of the salient image features. We then introduce the notion of ﬂow-guided anisotropic ﬁltering for detecting highly coherent lines while suppressing noise. Our method is simple and easy to implement. A variety of experimental results are presented to show the effectiveness of our method in producing self-contained, high-quality line illustrations.},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {Proceedings of the 5th international symposium on {Non}-photorealistic animation and rendering - {NPAR} '07},
	publisher = {ACM Press},
	author = {Kang, Henry and Lee, Seungyong and Chui, Charles K.},
	year = {2007},
	pages = {43},
	file = {Coherent Line Drawing.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Coherent Line Drawing.pdf:application/pdf},
}

@inproceedings{rosin_benchmarking_2017,
	address = {Los Angeles, California},
	title = {Benchmarking non-photorealistic rendering of portraits},
	isbn = {978-1-4503-5081-5},
	url = {http://dl.acm.org/citation.cfm?doid=3092919.3092921},
	doi = {10.1145/3092919.3092921},
	abstract = {We present a set of images for helping NPR practitioners evaluate their image-based portrait stylisation algorithms. Using a standard set both facilitates comparisons with other methods and helps ensure that presented results are representative. We give two levels of di culty, each consisting of 20 images selected systematically so as to provide good coverage of several possible portrait characteristics. We applied three existing portrait-speci c stylisation algorithms, two general-purpose stylisation algorithms, and one general learning based stylisation algorithm to the rst level of the benchmark, corresponding to the type of constrained images that have o en been used in portrait-speci c work. We found that the existing methods are generally e ective on this new image set, demonstrating that level one of the benchmark is tractable; challenges remain at level two. Results revealed several advantages conferred by portrait-speci c algorithms over general-purpose algorithms: portrait-speci c algorithms can use domain-speci c information to preserve key details such as eyes and to eliminate extraneous details, and they have more scope for semantically meaningful abstraction due to the underlying face model. Finally, we provide some thoughts on systematically extending the benchmark to higher levels of di culty.},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {Proceedings of the {Symposium} on {Non}-{Photorealistic} {Animation} and {Rendering} - {NPAR} '17},
	publisher = {ACM Press},
	author = {Rosin, Paul L. and Wang, Tinghuai and Winnemöller, Holger and Mould, David and Berger, Itamar and Collomosse, John and Lai, Yu-Kun and Li, Chuan and Li, Hua and Shamir, Ariel and Wand, Michael},
	year = {2017},
	pages = {1--12},
	file = {benchmarking npr rendering of portraits.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\benchmarking npr rendering of portraits.pdf:application/pdf},
}

@article{qian_gourd_2017,
	title = {Gourd pyrography art simulating based on non-photorealistic rendering},
	volume = {76},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-016-3801-8},
	doi = {10.1007/s11042-016-3801-8},
	language = {en},
	number = {13},
	urldate = {2021-12-01},
	journal = {Multimedia Tools and Applications},
	author = {Qian, Wenhua and Xu, Dan and Yue, Kun and Guan, Zheng and Pu, Yuanyuan and Shi, Yongjie},
	month = jul,
	year = {2017},
	pages = {14559--14579},
	file = {Qian2017_Article_GourdPyrographyArtSimulatingBa.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Qian2017_Article_GourdPyrographyArtSimulatingBa.pdf:application/pdf},
}

@article{kim_layered_2020,
	title = {Layered non-photorealistic rendering with anisotropic depth-of-field filtering},
	volume = {79},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-019-08387-2},
	doi = {10.1007/s11042-019-08387-2},
	abstract = {In this paper, we provide a layered non-photorealistic rendering (NPR) technique that automatically extracts the depth of field (DoF) shown in the picture and adjusts the degree of abstraction accordingly. We use an RGB channel to efficiently classify the DoF region anisotropically. Based on the DoF values, we abstract the color and adjust the thickness of the line. We use anisotropic DoF-based filtering to improve the abstraction quality by finding the blur region using cross-correlation filtering and anisotropically calculating the weight map. Our approach has greatly improved the quality of abstraction in terms of performance and design. The algorithm is also fast and simple to implement. Experimental results show well the characteristics and style of the DoF of the original photograph.},
	language = {en},
	number = {1-2},
	urldate = {2021-12-01},
	journal = {Multimedia Tools and Applications},
	author = {Kim, Jong-Hyun and Lee, Jung},
	month = jan,
	year = {2020},
	pages = {1291--1309},
	file = {Kim-Lee2020_Article_LayeredNon-photorealisticRende.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Kim-Lee2020_Article_LayeredNon-photorealisticRende.pdf:application/pdf},
}

@article{scalera_non-photorealistic_2019,
	title = {Non-{Photorealistic} {Rendering} {Techniques} for {Artistic} {Robotic} {Painting}},
	volume = {8},
	issn = {2218-6581},
	url = {http://www.mdpi.com/2218-6581/8/1/10},
	doi = {10.3390/robotics8010010},
	abstract = {In this paper, we present non-photorealistic rendering techniques that are applied together with a painting robot to realize artworks with original styles. Our robotic painting system is called Busker Robot and it has been considered of interest in recent art fairs and international exhibitions. It consists of a six degree-of-freedom collaborative robot and a series of image processing and path planning algorithms. In particular, here, two different rendering techniques are presented and a description of the experimental set-up is carried out. Finally, the experimental results are discussed by analyzing the elements that can account for the aesthetic appreciation of the artworks.},
	language = {en},
	number = {1},
	urldate = {2021-12-01},
	journal = {Robotics},
	author = {Scalera, Lorenzo and Seriani, Stefano and Gasparetto, Alessandro and Gallina, Paolo},
	month = feb,
	year = {2019},
	pages = {10},
	file = {NPR rendering artistic robotic painting.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\NPR rendering artistic robotic painting.pdf:application/pdf},
}

@article{kumar_comprehensive_2019,
	title = {A comprehensive survey on non-photorealistic rendering and benchmark developments for image abstraction and stylization},
	volume = {2},
	issn = {2520-8438, 2520-8446},
	url = {http://link.springer.com/10.1007/s42044-019-00034-1},
	doi = {10.1007/s42044-019-00034-1},
	abstract = {This survey presents a comprehensive study on non-photorealistic rendering (NPR). NPR technique renders 2D input image into abstracted and artistic stylized images. NPR mainly dwells on image processing, computer vision, and visualizing the graphics processing techniques. The survey highlights the evolution of IA–AR system and the subsequent classiﬁcation of NPR techniques. The survey also has cognized the various works done on stroke-based rendering, color image analogy, region-based rendering, image ﬁltering abstraction, and stylization. The inference drawn from the survey is the computer system using a stylus to fully automatic structure-preserving image abstraction and stylization got evolved from a traditional method of human interaction. From the survey carried out on the most signiﬁcant papers from 1963 to 2017, we felt the need for setting up of benchmark guidelines, the data set with varies subjective matters, and quality assessment techniques with various statistical essences. From the survey information pertaining to benchmark image characteristics, properties and their constraints with the contextual feature in an image have been identiﬁed. Finally, survey work listed out the NPR application in various ﬁelds of image processing and highlighted empirical challenges and hampers in NPR domain. This survey work has empowered us to proceed in the right direction and enthusiasm to bring forth the problem statement and carry out research work.},
	language = {en},
	number = {3},
	urldate = {2021-12-01},
	journal = {Iran Journal of Computer Science},
	author = {Kumar, M. P. Pavan and Poornima, B. and Nagendraswamy, H. S. and Manjunath, C.},
	month = sep,
	year = {2019},
	pages = {131--165},
	file = {Kumar2019_Article_AComprehensiveSurveyOnNon-phot.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Kumar2019_Article_AComprehensiveSurveyOnNon-phot.pdf:application/pdf},
}

@article{gooch_human_2004,
	title = {Human facial illustrations: {Creation} and psychophysical evaluation},
	volume = {23},
	issn = {0730-0301, 1557-7368},
	shorttitle = {Human facial illustrations},
	url = {https://dl.acm.org/doi/10.1145/966131.966133},
	doi = {10.1145/966131.966133},
	abstract = {We present a method for creating black-and-white illustrations from photographs of human faces. In addition an interactive technique is demonstrated for deforming these black-and-white facial illustrations to create caricatures which highlight and exaggerate representative facial features. We evaluate the effectiveness of the resulting images through psychophysical studies to assess accuracy and speed in both recognition and learning tasks. These studies show that the facial illustrations and caricatures generated using our techniques are as effective as photographs in recognition tasks. For the learning task we find that illustrations are learned two times faster than photographs and caricatures are learned one and a half times faster than photographs. Because our techniques produce images that are effective at communicating complex information, they are useful in a number of potential applications, ranging from entertainment and education to low bandwidth telecommunications and psychology research.},
	language = {en},
	number = {1},
	urldate = {2021-12-01},
	journal = {ACM Transactions on Graphics},
	author = {Gooch, Bruce and Reinhard, Erik and Gooch, Amy},
	month = jan,
	year = {2004},
	pages = {27--44},
	file = {human facial illustrations.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\human facial illustrations.pdf:application/pdf},
}

@inproceedings{salisbury_interactive_1994,
	address = {Not Known},
	title = {Interactive pen-and-ink illustration},
	isbn = {978-0-89791-667-7},
	url = {http://portal.acm.org/citation.cfm?doid=192161.192185},
	doi = {10.1145/192161.192185},
	abstract = {We present an interactive system for creating pen-and-ink illustrations. The system uses stroke textures—collections of strokes arranged in different patterns—to generate texture and tone. The user “paints” with a desired stroke texture to achieve a desired tone, and the computer draws all of the individual strokes.},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {Proceedings of the 21st annual conference on {Computer} graphics and interactive techniques  - {SIGGRAPH} '94},
	publisher = {ACM Press},
	author = {Salisbury, Michael P. and Anderson, Sean E. and Barzel, Ronen and Salesin, David H.},
	year = {1994},
	pages = {101--108},
	file = {Interactive Pen and Ink Illustration.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\Interactive Pen and Ink Illustration.pdf:application/pdf},
}

@inproceedings{colton_emotionally_2008,
	address = {New York, NY, USA},
	series = {{DIMEA} '08},
	title = {Emotionally aware automated portrait painting},
	isbn = {978-1-60558-248-1},
	url = {https://doi.org/10.1145/1413634.1413690},
	doi = {10.1145/1413634.1413690},
	abstract = {We combine a machine vision system that recognises emotions and a non-photorealistic rendering (NPR) system to automatically produce portraits which heighten the emotion of the sitter. To do this, the vision system analyses a short video clip of a person expressing an emotion, then tracks the movement of facial features and uses this tracking data to analyse which emotion was expressed and what the temporal dynamics of the expression were. The image where the emotion is expressed strongest, the location of the facial features in that image and a keyword describing the emotion detected are passed to the NPR software. This keyword is used to choose appropriate (simulated) art materials, colour palettes, abstraction methods and painting styles, so that the rendered image may heighten the emotion being expressed. We describe the vision and rendering systems and their combination, and provide examples of portraits produced in this emotionally aware fashion.},
	urldate = {2021-12-01},
	booktitle = {Proceedings of the 3rd international conference on {Digital} {Interactive} {Media} in {Entertainment} and {Arts}},
	publisher = {Association for Computing Machinery},
	author = {Colton, Simon and Valstar, Michel F. and Pantic, Maja},
	month = sep,
	year = {2008},
	keywords = {affective computing, artificial creativity, automatic facial expression recognition, computational creativity, emotion detection, machine vision, non-photorealistic rendering},
	pages = {304--311},
}

@inproceedings{shugrina_empathic_2006,
	address = {New York, NY, USA},
	series = {{NPAR} '06},
	title = {Empathic painting: interactive stylization through observed emotional state},
	isbn = {978-1-59593-357-7},
	shorttitle = {Empathic painting},
	url = {https://doi.org/10.1145/1124728.1124744},
	doi = {10.1145/1124728.1124744},
	abstract = {We present the "empathic painting" --- an interactive painterly rendering whose appearance adapts in real time to reflect the perceived emotional state of the viewer. The empathic painting is an experiment into the feasibility of using high level control parameters (namely, emotional state) to replace the plethora of low-level constraints users must typically set to affect the output of artistic rendering algorithms. We describe a suite of Computer Vision algorithms capable of recognising users' facial expressions through the detection of facial action units derived from the FACS scheme. Action units are mapped to vectors within a continuous 2D space representing emotional state, from which we in turn derive a continuous mapping to the style parameters of a simple but fast segmentation-based painterly rendering algorithm. The result is a digital canvas capable of smoothly varying its painterly style at approximately 4 frames per second, providing a novel user interactive experience using only commodity hardware.},
	urldate = {2021-12-01},
	booktitle = {Proceedings of the 4th international symposium on {Non}-photorealistic animation and rendering},
	publisher = {Association for Computing Machinery},
	author = {Shugrina, Maria and Betke, Margrit and Collomosse, John},
	month = jun,
	year = {2006},
	keywords = {animation, emotion, FACS, painterly rendering},
	pages = {87--96},
}

@inproceedings{setlur_automatic_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Automatic {Stained} {Glass} {Rendering}},
	isbn = {978-3-540-35639-4},
	doi = {10.1007/11784203_66},
	abstract = {Based on artistic techniques for the creation of stained glass, we introduce a method to automatically create images in stained glass stylization of images. Our algorithm first applies segmentation, and performs region simplification to merge and simplify the segments. The system then queries a database of glass swatch images and computes an optimal matching subset based on color and texture metrics. These swatches are then mapped onto the original image and 3D rendering effects including normal mapping, translucency, lead came and refraction are applied to generate the stained glass output.},
	language = {en},
	booktitle = {Advances in {Computer} {Graphics}},
	publisher = {Springer},
	author = {Setlur, Vidya and Wilkinson, Stephen},
	editor = {Nishita, Tomoyuki and Peng, Qunsheng and Seidel, Hans-Peter},
	year = {2006},
	keywords = {Histogram Intersection, Image Segment, Interactive Technique, Texture Feature, Texture Synthesis},
	pages = {682--691},
}

@misc{noauthor_color_nodate,
	title = {Color {Quantization}},
	url = {https://www.mathworks.com/matlabcentral/fileexchange/31687-color-quantization},
	abstract = {This program reduces the number of colors present in a true color image (or indexed color image).},
	language = {en},
	urldate = {2021-12-20},
	file = {Snapshot:C\:\\Users\\Digital Traders\\Zotero\\storage\\9FQZ98FA\\31687-color-quantization.html:text/html},
}

@misc{noauthor_segment_nodate,
	title = {Segment image into foreground and background using graph-based segmentation - {MATLAB} lazysnapping},
	url = {https://www.mathworks.com/help/images/ref/lazysnapping.html?searchHighlight=lazy%20snapping&s_tid=srchtitle_lazy%20snapping_4},
	urldate = {2021-12-20},
	file = {Segment image into foreground and background using graph-based segmentation - MATLAB lazysnapping:C\:\\Users\\Digital Traders\\Zotero\\storage\\T5T8V4RK\\lazysnapping.html:text/html},
}

@misc{noauthor_lazysnapping_10157061015719pdf_nodate,
	title = {lazysnapping\_1015706.1015719.pdf},
	file = {lazysnapping_1015706.1015719.pdf:D\:\\OneDrive - Habib University\\Fall 2021\\Digital Image Processing\\Project\\Resources\\lazysnapping_1015706.1015719.pdf:application/pdf},
}

@article{li_lazy_2004,
	title = {Lazy {Snapping}},
	volume = {23},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1015706.1015719},
	doi = {10.1145/1015706.1015719},
	abstract = {In this paper, we present Lazy Snapping, an interactive image cutout tool. Lazy Snapping separates coarse and fine scale processing, making object specification and detailed adjustment easy. Moreover, Lazy Snapping provides instant visual feedback, snapping the cutout contour to the true object boundary efficiently despite the presence of ambiguous or low contrast edges. Instant feedback is made possible by a novel image segmentation algorithm which combines graph cut with pre-computed over-segmentation. A set of intuitive user interface (UI) tools is designed and implemented to provide flexible control and editing for the users. Usability studies indicate that Lazy Snapping provides a better user experience and produces better segmentation results than the state-of-the-art interactive image cutout tool, Magnetic Lasso in Adobe Photoshop.},
	number = {3},
	journal = {ACM Trans. Graph.},
	author = {Li, Yin and Sun, Jian and Tang, Chi-Keung and Shum, Heung-Yeung},
	month = aug,
	year = {2004},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Graph Cut, Image Cutout, Interactive Image Segmentation, User Interface},
	pages = {303--308},
}

@misc{noauthor_segment_nodate-1,
	title = {Segment image into two or three regions using geodesic distance-based color segmentation - {MATLAB} imseggeodesic},
	url = {https://www.mathworks.com/help/images/ref/imseggeodesic.html?s_tid=doc_ta},
	urldate = {2021-12-20},
	file = {Segment image into two or three regions using geodesic distance-based color segmentation - MATLAB imseggeodesic:C\:\\Users\\Digital Traders\\Zotero\\storage\\TSAC7CWN\\imseggeodesic.html:text/html},
}

@article{protiere_interactive_2007,
	title = {Interactive {Image} {Segmentation} via {Adaptive} {Weighted} {Distances}},
	volume = {16},
	issn = {1057-7149},
	url = {https://doi.org/10.1109/TIP.2007.891796},
	doi = {10.1109/TIP.2007.891796},
	abstract = {An interactive algorithm for soft segmentation of natural images is presented in this paper. The user first roughly scribbles different regions of interest, and from them, the whole image is automatically segmented. This soft segmentation is obtained via fast, linear complexity computation of weighted distances to the user-provided scribbles. The adaptive weights are obtained from a series of Gabor filters, and are automatically computed according to the ability of each single filter to discriminate between the selected regions of interest. We present the underlying framework and examples showing the capability of the algorithm to segment diverse images},
	number = {4},
	journal = {Trans. Img. Proc.},
	author = {Protiere, A. and Sapiro, G.},
	month = apr,
	year = {2007},
	note = {Publisher: IEEE Press},
	keywords = {Adaptive weights, distance functions, interactive segmentation, linear complexity, natural images},
	pages = {1046--1057},
}

@misc{noauthor_k-means_nodate,
	title = {K-means clustering based image segmentation - {MATLAB} imsegkmeans},
	url = {https://www.mathworks.com/help/images/ref/imsegkmeans.html?s_tid=doc_ta},
	urldate = {2021-12-20},
	file = {K-means clustering based image segmentation - MATLAB imsegkmeans:C\:\\Users\\Digital Traders\\Zotero\\storage\\U4HG5ZI6\\imsegkmeans.html:text/html},
}

@article{arthur_k-means_nodate,
	title = {k-means++: {The} {Advantages} of {Careful} {Seeding}},
	abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it oﬀers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(log k)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	language = {en},
	author = {Arthur, David and Vassilvitskii, Sergei},
	pages = {9},
	file = {Arthur and Vassilvitskii - k-means++ The Advantages of Careful Seeding.pdf:C\:\\Users\\Digital Traders\\Zotero\\storage\\7YMU25PI\\Arthur and Vassilvitskii - k-means++ The Advantages of Careful Seeding.pdf:application/pdf},
}

@misc{noauthor_bilateral_nodate,
	title = {Bilateral filtering of images with {Gaussian} kernels - {MATLAB} imbilatfilt},
	url = {https://www.mathworks.com/help/images/ref/imbilatfilt.html?docviewer=null},
	urldate = {2021-12-20},
	file = {Bilateral filtering of images with Gaussian kernels - MATLAB imbilatfilt:C\:\\Users\\Digital Traders\\Zotero\\storage\\HS3ZNVCL\\imbilatfilt.html:text/html},
}

@inproceedings{tomasi_bilateral_1998,
	address = {Bombay, India},
	title = {Bilateral filtering for gray and color images},
	isbn = {978-81-7319-221-0},
	url = {http://ieeexplore.ieee.org/document/710815/},
	doi = {10.1109/ICCV.1998.710815},
	abstract = {Bilateral ﬁltering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. In contrast with ﬁlters that operate on the three bands of a color image separately, a bilateral ﬁlter can enforce the perceptual metric underlying the CIE-Lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. Also, in contrast with standard ﬁltering, bilateral ﬁltering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {Sixth {International} {Conference} on {Computer} {Vision} ({IEEE} {Cat}. {No}.{98CH36271})},
	publisher = {Narosa Publishing House},
	author = {Tomasi, C. and Manduchi, R.},
	year = {1998},
	pages = {839--846},
	file = {Tomasi and Manduchi - 1998 - Bilateral filtering for gray and color images.pdf:C\:\\Users\\Digital Traders\\Zotero\\storage\\KTWBGMII\\Tomasi and Manduchi - 1998 - Bilateral filtering for gray and color images.pdf:application/pdf},
}

@misc{noauthor_convert_nodate,
	title = {Convert {RGB} image to indexed image - {MATLAB} rgb2ind},
	url = {https://www.mathworks.com/help/matlab/ref/rgb2ind.html?s_tid=doc_ta},
	urldate = {2021-12-20},
	file = {Convert RGB image to indexed image - MATLAB rgb2ind:C\:\\Users\\Digital Traders\\Zotero\\storage\\H4NM54AH\\rgb2ind.html:text/html},
}

@misc{noauthor_multilevel_nodate,
	title = {Multilevel image thresholds using {Otsu}’s method - {MATLAB} multithresh},
	url = {https://www.mathworks.com/help/images/ref/multithresh.html?s_tid=doc_ta},
	urldate = {2021-12-20},
	file = {Multilevel image thresholds using Otsu’s method - MATLAB multithresh:C\:\\Users\\Digital Traders\\Zotero\\storage\\9MPYUQVU\\multithresh.html:text/html},
}

@misc{noauthor_quantize_nodate,
	title = {Quantize image using specified quantization levels and output values - {MATLAB} imquantize},
	url = {https://www.mathworks.com/help/images/ref/imquantize.html?s_tid=doc_ta},
	urldate = {2021-12-20},
	file = {Quantize image using specified quantization levels and output values - MATLAB imquantize:C\:\\Users\\Digital Traders\\Zotero\\storage\\3BQCL6LH\\imquantize.html:text/html},
}